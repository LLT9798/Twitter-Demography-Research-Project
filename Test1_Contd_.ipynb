{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Test1_Contd..ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LLT9798/Twitter-Demography-Research-Project/blob/master/Test1_Contd_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2TnZx8fDWpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75d6c753-ebf4-45dc-9ae8-202074302a3c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import six\n",
        "from abc import ABCMeta\n",
        "from scipy import sparse\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils import check_X_y, check_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "print(tf.__version__)\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkJ0YzHkDlQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "cellView": "both",
        "outputId": "5b1d72ff-94d0-4ea6-e092-92e741ce3326"
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "# Get data from Google drive\n",
        "link = 'https://drive.google.com/open?id=1A1sv29Np75mWPhz9nVk35UptWDztNHp3'\n",
        "\n",
        "fluff, id = link.split('=')\n",
        "print (id)\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('791531.csv')  \n",
        "\n",
        "df = pd.read_csv('791531.csv', encoding='latin-1')\n",
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1A1sv29Np75mWPhz9nVk35UptWDztNHp3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10020, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M4pZ5-UDWqM",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "df =df.drop(columns='Unnamed: 0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO9WHDoy-ave",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "31f98f99-dd5b-4e10-ad00-6c87bcf55658"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ÂÃÃIt felt like they were my friends and I ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ive seen people on the train with lamps, chair...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gala Bingo clubs bought for Ã¥Â£241m: The UK's...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@_Aphmau_ the pic defines all mcd fangirls/fan...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@Evielady just how lovely is the tree this yea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>https://t.co/EROGWTFTYo It's a glow of satisfa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@giannaaa28 lmao _ÃÃ·Ã¢_ÃÃ·Ã¢ dude I'm hella...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  gender\n",
              "0  Robbie E Responds To Critics After Win Against...       1\n",
              "1  ÂÃÃIt felt like they were my friends and I ...       1\n",
              "2  Hi @JordanSpieth - Looking at the url - do you...       1\n",
              "3  Watching Neighbours on Sky+ catching up with t...       0\n",
              "4  Ive seen people on the train with lamps, chair...       0\n",
              "5  Gala Bingo clubs bought for Ã¥Â£241m: The UK's...       1\n",
              "6  @_Aphmau_ the pic defines all mcd fangirls/fan...       0\n",
              "7  @Evielady just how lovely is the tree this yea...       0\n",
              "8  https://t.co/EROGWTFTYo It's a glow of satisfa...       0\n",
              "9  @giannaaa28 lmao _ÃÃ·Ã¢_ÃÃ·Ã¢ dude I'm hella...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXph_Uam_OzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_wordlist( review, remove_stopwords=True):\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
        "\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "\n",
        "    # 4. Optionally remove stop words (Apply NLTK)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    b=[]\n",
        "    stemmer = english_stemmer #PorterStemmer()\n",
        "    for word in words:\n",
        "        b.append(stemmer.stem(word))\n",
        "\n",
        "    # 5. Return a list of words\n",
        "    return(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAz3brVh_YM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(df, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI9DsHvAAkDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_train_text = []\n",
        "for text in train['text']:\n",
        "    clean_train_text.append( \" \".join(to_wordlist(text)))\n",
        "    \n",
        "clean_test_text = []\n",
        "for text in test['text']:\n",
        "    clean_test_text.append( \" \".join(to_wordlist(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UFHiGEJBIBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer( min_df=2, max_df=0.95, max_features = 200000, ngram_range = ( 1, 4 ))\n",
        "\n",
        "vectorizer = vectorizer.fit(clean_train_text)\n",
        "train_features = vectorizer.transform(clean_train_text)\n",
        "\n",
        "test_features = vectorizer.transform(clean_test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZREkLhf4DJXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fselect = SelectKBest(chi2 , k=10000)\n",
        "train_features = fselect.fit_transform(train_features, train['gender'])\n",
        "test_features = fselect.transform(test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJGB4mPYBciS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = MultinomialNB(alpha=0.001)\n",
        "model1.fit( train_features, train['gender'] )\n",
        "\n",
        "model2 = SGDClassifier(loss='modified_huber', random_state=100, shuffle=True)\n",
        "model2.fit( train_features, train['gender'] )\n",
        "\n",
        "model3 = RandomForestClassifier()\n",
        "model3.fit( train_features, train['gender'] )\n",
        "\n",
        "model4 = GradientBoostingClassifier()\n",
        "model4.fit( train_features, train['gender'] )\n",
        "\n",
        "pred_1 = model1.predict( test_features.toarray() )\n",
        "pred_2 = model2.predict( test_features.toarray() )\n",
        "pred_3 = model3.predict( test_features.toarray() )\n",
        "pred_4 = model4.predict( test_features.toarray() )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm2McTLsB8kl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "192a36df-0aa1-4979-9a76-8aa63949e538"
      },
      "source": [
        "print('prediction 1 accuracy: ', accuracy_score(test['gender'], pred_1))\n",
        "print('prediction 2 accuracy: ', accuracy_score(test['gender'], pred_2))\n",
        "print('prediction 3 accuracy: ', accuracy_score(test['gender'], pred_3))\n",
        "print('prediction 4 accuracy: ', accuracy_score(test['gender'], pred_4))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction 1 accuracy:  0.5863273453093812\n",
            "prediction 2 accuracy:  0.5788423153692615\n",
            "prediction 3 accuracy:  0.5873253493013972\n",
            "prediction 4 accuracy:  0.5643712574850299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7NQ2RpOD1tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gccUPnzOD8t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "nb_classes = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfDPT-JnD9lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 1000, ngram_range = ( 1, 4 ))\n",
        "\n",
        "vectorizer = vectorizer.fit(clean_train_text)\n",
        "train_features = vectorizer.transform(clean_train_text)\n",
        "\n",
        "test_features = vectorizer.transform(clean_test_text)\n",
        "\n",
        "X_train = train_features.toarray()\n",
        "X_test = test_features.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B-BNXZvEK5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2952781a-e417-4d54-d6b2-56497324662a"
      },
      "source": [
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (8016, 1000)\n",
            "X_test shape: (2004, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4wTD7hHE-HM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "cf88daeb-0555-463f-8b31-2496112a96e7"
      },
      "source": [
        "np.array(train['text'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['We love it when somebody states the obvious! Although it is true... https://t.co/vEg1bq4y88',\n",
              "       \"Ms. Despar is taking us on a field trip to the hospital she works at and I'm going to dress up in scrubs and sneak in to perform a surgery\",\n",
              "       'NFL: #Dolphins Dominate the #Texans - https://t.co/ZcfA6lYI8q',\n",
              "       ...,\n",
              "       'Dunno wats worse, me waking into my sons room and he\\'s doing \"waists movement\" for the TL or this  https://t.co/0057BOWKey',\n",
              "       '@rareblackjak and that list is long..&amp; ignored by many. Why is dignity an afterthought on social media? Is attention so important?',\n",
              "       'I get the sudden urge to chop off my hair and dye it black and get a bunch of tattoos _Ã\\x99Ã·Â\\x90_Ã\\x99Ã·Ã\\x89 #yeahright'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hVahu-EXQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0a6f1b5-92a6-4d23-ef36-7ddb9945fd08"
      },
      "source": [
        "y_train = np.array(train['gender']-1)\n",
        "y_test = np.array(test['gender']-1)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "\n",
        "# pre-processing: divide by max and substract mean\n",
        "scale = np.max(X_train)\n",
        "X_train /= scale\n",
        "X_test /= scale\n",
        "\n",
        "mean = np.mean(X_train)\n",
        "X_train -= mean\n",
        "X_test -= mean\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Here's a Deep Dumb MLP (DDMLP)\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=input_dim))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "optimizer = optimizers.SGD(lr=0.25, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train, Y_train, nb_epoch= 30, batch_size=30, validation_split=0.1)\n",
        "\n",
        "preds = model.predict_classes(X_test, verbose=0)\n",
        "print(preds)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7214 samples, validate on 802 samples\n",
            "Epoch 1/30\n",
            "7214/7214 [==============================] - 3s 409us/step - loss: 0.6992 - acc: 0.5266 - val_loss: 0.7028 - val_acc: 0.5224\n",
            "Epoch 2/30\n",
            "7214/7214 [==============================] - 1s 202us/step - loss: 0.6980 - acc: 0.5093 - val_loss: 0.6960 - val_acc: 0.5224\n",
            "Epoch 3/30\n",
            "7214/7214 [==============================] - 1s 198us/step - loss: 0.6961 - acc: 0.5238 - val_loss: 0.7215 - val_acc: 0.5224\n",
            "Epoch 4/30\n",
            "7214/7214 [==============================] - 1s 202us/step - loss: 0.6950 - acc: 0.5241 - val_loss: 0.6922 - val_acc: 0.5224\n",
            "Epoch 5/30\n",
            "7214/7214 [==============================] - 1s 196us/step - loss: 0.6943 - acc: 0.5205 - val_loss: 0.6932 - val_acc: 0.5224\n",
            "Epoch 6/30\n",
            "7214/7214 [==============================] - 1s 196us/step - loss: 0.6975 - acc: 0.5193 - val_loss: 0.6924 - val_acc: 0.5224\n",
            "Epoch 7/30\n",
            "7214/7214 [==============================] - 1s 201us/step - loss: 0.6961 - acc: 0.5266 - val_loss: 0.7064 - val_acc: 0.5224\n",
            "Epoch 8/30\n",
            "7214/7214 [==============================] - 1s 198us/step - loss: 0.6974 - acc: 0.5169 - val_loss: 0.6985 - val_acc: 0.4776\n",
            "Epoch 9/30\n",
            "7214/7214 [==============================] - 2s 215us/step - loss: 0.6966 - acc: 0.5177 - val_loss: 0.6922 - val_acc: 0.5224\n",
            "Epoch 10/30\n",
            "7214/7214 [==============================] - 1s 204us/step - loss: 0.6963 - acc: 0.5237 - val_loss: 0.6923 - val_acc: 0.5224\n",
            "Epoch 11/30\n",
            "7214/7214 [==============================] - 2s 216us/step - loss: 0.6975 - acc: 0.5152 - val_loss: 0.6950 - val_acc: 0.4776\n",
            "Epoch 12/30\n",
            "7214/7214 [==============================] - 2s 225us/step - loss: 0.6956 - acc: 0.5222 - val_loss: 0.7128 - val_acc: 0.4776\n",
            "Epoch 13/30\n",
            "7214/7214 [==============================] - 2s 217us/step - loss: 0.6961 - acc: 0.5211 - val_loss: 0.6968 - val_acc: 0.4776\n",
            "Epoch 14/30\n",
            "7214/7214 [==============================] - 2s 232us/step - loss: 0.6979 - acc: 0.5132 - val_loss: 0.6960 - val_acc: 0.4776\n",
            "Epoch 15/30\n",
            "7214/7214 [==============================] - 2s 231us/step - loss: 0.6965 - acc: 0.5208 - val_loss: 0.6923 - val_acc: 0.5224\n",
            "Epoch 16/30\n",
            "7214/7214 [==============================] - 2s 221us/step - loss: 0.6961 - acc: 0.5254 - val_loss: 0.7003 - val_acc: 0.5224\n",
            "Epoch 17/30\n",
            "7214/7214 [==============================] - 1s 205us/step - loss: 0.6956 - acc: 0.5205 - val_loss: 0.6930 - val_acc: 0.5224\n",
            "Epoch 18/30\n",
            "7214/7214 [==============================] - 1s 194us/step - loss: 0.6965 - acc: 0.5176 - val_loss: 0.6944 - val_acc: 0.5224\n",
            "Epoch 19/30\n",
            "7214/7214 [==============================] - 1s 187us/step - loss: 0.6979 - acc: 0.5139 - val_loss: 0.7130 - val_acc: 0.5224\n",
            "Epoch 20/30\n",
            "7214/7214 [==============================] - 1s 193us/step - loss: 0.6979 - acc: 0.5189 - val_loss: 0.6937 - val_acc: 0.5224\n",
            "Epoch 21/30\n",
            "7214/7214 [==============================] - 2s 230us/step - loss: 0.6961 - acc: 0.5162 - val_loss: 0.6922 - val_acc: 0.5224\n",
            "Epoch 22/30\n",
            "7214/7214 [==============================] - 2s 293us/step - loss: 0.6948 - acc: 0.5266 - val_loss: 0.6937 - val_acc: 0.5224\n",
            "Epoch 23/30\n",
            "7214/7214 [==============================] - 1s 196us/step - loss: 0.6963 - acc: 0.5211 - val_loss: 0.6927 - val_acc: 0.5224\n",
            "Epoch 24/30\n",
            "7214/7214 [==============================] - 1s 193us/step - loss: 0.6951 - acc: 0.5216 - val_loss: 0.7054 - val_acc: 0.5224\n",
            "Epoch 25/30\n",
            "7214/7214 [==============================] - 1s 199us/step - loss: 0.6970 - acc: 0.5205 - val_loss: 0.6930 - val_acc: 0.5224\n",
            "Epoch 26/30\n",
            "7214/7214 [==============================] - 1s 185us/step - loss: 0.6951 - acc: 0.5195 - val_loss: 0.7025 - val_acc: 0.5224\n",
            "Epoch 27/30\n",
            "7214/7214 [==============================] - 1s 183us/step - loss: 0.6958 - acc: 0.5225 - val_loss: 0.6929 - val_acc: 0.5224\n",
            "Epoch 28/30\n",
            "7214/7214 [==============================] - 1s 184us/step - loss: 0.6952 - acc: 0.5255 - val_loss: 0.7004 - val_acc: 0.5224\n",
            "Epoch 29/30\n",
            "7214/7214 [==============================] - 1s 183us/step - loss: 0.6962 - acc: 0.5183 - val_loss: 0.6983 - val_acc: 0.5224\n",
            "Epoch 30/30\n",
            "7214/7214 [==============================] - 1s 201us/step - loss: 0.6969 - acc: 0.5200 - val_loss: 0.6924 - val_acc: 0.5224\n",
            "[1 1 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoY6IHgwFEIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02df351a-cff4-48ae-97f7-3425f2a091fb"
      },
      "source": [
        "print('prediction 5 accuracy: ', accuracy_score(test['gender'], preds))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction 5 accuracy:  0.4785429141716567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n1aOG_aJ0Yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e2205d2-da02-40c8-aadc-40ff837be1c6"
      },
      "source": [
        "metrics.f1_score(test['gender'], preds, average='weighted')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30976891980467014"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DrMAiitQWYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}